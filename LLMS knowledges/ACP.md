> [!tip] 阿里云大模型ACP高级工程师认证（Alibaba Cloud Certified Professional - LLM）
> 
> 1. 能力认可：官方认证背书，证明大模型解决方案设计与落地能力；
> 2. 生态适配：深度绑定阿里百炼平台，适配阿里系企业及合作伙伴的人才需求；
> 3. 技能提升：通过培训考试，系统掌握百炼平台高阶功能及复杂场景应对技巧。


# 1. 上下文工程 (Context Engineering) 

> [!info] 核心技术
> **RAG (检索增强生成)**：从外部知识库（如公司文档）中**检索**信息，为模型提供精准的回答依据。
> **Prompt (提示词工程)**：通过精心设计的**指令**，精确地引导模型的思考方式和输出格式。
> **Tool (工具使用)**：赋予模型调用外部**工具**（如计算器、搜索引擎、API）的能力，以获取实时信息或执行特定任务。
> **Memory (记忆机制)**：为模型建立长短期**记忆**，使其能够在连续对话中理解历史上下文。

## 1.1 RAG（检索增强生成）

**RAG（Retrieval-Augmented Generation，检索增强生成）** 就是实现上下文工程的强大技术方案。它的核心思想是：解决大模型的知识局限性。

在用户提问时，不再将全部知识库硬塞给大模型，而是先**自动检索**出与问题最相关的私有知识片段，然后将这些精准的片段与用户问题合并后，一同传给大模型，从而**生成**最终的答案。这样既避免了提示词过长的问题，又能确保大模型获得相关的背景信息。

构建一个 RAG 应用通常会分为两个阶段

### 1.1.1 第一阶段：建立索引
<img src="https://gw.alicdn.com/imgextra/i2/O1CN010zLf411zVoZQ9cWsI_!!6000000006720-2-tps-1592-503.png" width="600">

建立索引是为了将私有知识文档或片段转换为可以高效检索的形式。通过将文件内容分割并转化为多维向量（使用专用 Embedding 模型），并结合向量存储保留文本的语义信息，方便进行相似度计算。向量化使得模型能够高效检索和匹配相关内容，特别是在处理大规模知识库时，显著提高了查询的准确性和响应速度。

这些向量经过 Embedding 模型处理后不仅很好地捕捉文本内容的语义信息，而且由于语义已经向量化，标准化，便于之后与检索语义向量进行相关度计算。

#### 步骤
1. **文档解析**  
    就像你会将书上看到的视觉信息理解为文字信息一样，RAG应用也需要首先将知识库文档进行加载并解析为大模型能够理解的文字形式。
    
2. **文本分段**  
    你通常不会在做某道题时把整本书都翻阅一遍，而是去查找与问题最相关的几个段落，因此你会先把参考资料做一个大致的分段。类似的，RAG应用也会在文档解析后对文本进行分段，以便于在后续能够快速找到与提问最相关的内容。
    
3. **文本向量化**  
    在开卷考试时，你通常会先在参考资料中寻找与问题最相关的段落，再去进行作答。在RAG应用中，通常需要借助嵌入（embedding）模型分别对段落与问题进行数字化表示，在进行相似度比较后找出最相关的段落，数字化表示的过程就叫做文本向量化。  
    
    > [!tip] 计算机并不能直接理解“我喜欢吃苹果”与“我爱吃苹果”这两句话到底有多相似，但它能理解两个相同维度的向量的相似度（通常使用余弦相似度来衡量）。文本向量化通过embedding模型，将自然语言转化为计算机能够理解的数字形式。
    > 1. embedding模型的训练通常会包含**对比学习**的环节，输入数据是许多已标记为是否相关的文本对(s1,s2)，模型的训练目标是尽可能让相关的文本对生成的向量相似度变高，不相关的文本对生成的向量相似度变低。
    > 2. 在**建立索引**阶段，假设已经通过文本分段获得了n个chunk：[c1,c2,c3,…,cn]，那么embedding模型会将这n个chunk分别转化为向量：[v1,v2,v3,…,vn]，并存储为向量数据库。
    > 3. 在**检索**阶段，假设用户的问题为q，那么embedding模型会将问题q转化为向量vq，并在向量数据库中找出与vq最相似的n个向量（这个值你可以自己设定），通过向量与文本段的索引关系得到文本段，作为检索结果。
    
4. **存储索引**  
    存储索引将向量化后的段落存储为向量数据库，这样RAG应用就无需在每次进行回复时都重复以上步骤，从而可以增加响应速度。
    
    <img src="https://gw.alicdn.com/imgextra/i2/O1CN010zLf411zVoZQ9cWsI_!!6000000006720-2-tps-1592-503.png" width="600"><br>

    在建立索引后，RAG应用就可以根据用户的问题检索出相关的文本段了。
    

### 1.1.2 第二阶段：检索与生成
<img src="https://img.alicdn.com/imgextra/i1/O1CN01vbkBXC1HQ0SBrC1Ii_!!6000000000751-2-tps-1776-639.png" width="600">

检索生成是根据用户的提问，从索引中检索相关的文档片段，这些片段会与提问一起输入到大模型生成最终的回答。这样大模型就能够回答私有知识问题了。

总的来说，基于 RAG 结构的应用，既避免了将整个参考文档作为背景信息输入而导致的各种问题，又通过检索提取出了与问题最相关的部分，从而提高了大模型输出的准确性与相关性。

#### 步骤
检索、生成分别对应着RAG名字中的`Retrieval`与`Generation`两阶段。**检索**就像开卷考试时去查找资料的过程，**生成**则是在找到资料后，根据参考资料与问题进行作答的过程。

1. **检索**
检索阶段会召回与问题最相关的文本段。通过embedding模型对问题进行文本向量化，并与向量数据库的段落进行语义相似度的比较，找出最相关的段落。检索是RAG应用中最重要的环节，你可以想象如果考试的时候找到了错误的资料，那么回答一定是不准确的。这个步骤完美诠释了上下文工程的精髓：从海量知识中“精准地选择相关信息”来填充上下文。找到最匹配的内容，是保证后续生成质量的第一步。为了提高检索准确性，除了使用性能强大的embedding模型，也可以做重排（rerank）、句子窗口检索等方法，这些内容你可以在之后的章节进行学习。
2. **生成**
在检索到相关的文本段后，RAG应用会将问题与文本段通过提示词模板生成最终的提示词，由大模型生成回复，这个阶段更多是利用大模型的总结能力，而不是大模型本身具有的知识。这个提示词模板的设计，是上下文工程的另一个关键环节。我们不仅要提供检索到的“资料”，还要明确地“指导”模型如何使用这些资料来回答问题。
    > 一个典型的提示词模板为：`请根据以下信息回答用户的问题：{召回文本段}。用户的问题是：{question}。`

    <img src="https://img.alicdn.com/imgextra/i1/O1CN01vbkBXC1HQ0SBrC1Ii_!!6000000000751-2-tps-1776-639.png" width="600"><br>


### 整体流程
<img src="https://img.alicdn.com/imgextra/i1/O1CN01GjUPUs1dIbaxdDowf_!!6000000003713-2-tps-1971-1221.png" width="600" />

1. **文档解析**
就像你将书上看到的信息理解为文字信息一样，RAG应用也需要首先将知识库文档进行加载并解析为大模型能够理解的文字形式；

2. **文本分段**
你通常不会在做某道题时把整本书都翻阅一遍，而是查找与问题最相关的几个段落，分段方法可能是按照目录中的标题等方法，RAG应用也会在解析后将文本分段，以便于后续的检索。

3. **索引建立**
在开卷考试时，通常会先找到与问题最相关的几个段落，然后根据这些段落进行回答。RAG应用也会使用嵌入模型（embedding 模型）对切片后的文本段建立索引，并以向量数据库形式存储，便于后续检索。
使用嵌入模型（embedding 模型）对切片后的文档建立索引，并以向量数据库形式存储，便于后续检索；

4. 发起提问：用户输入的问题与向量数据库的知识进行匹配，并通过大模型生成结合知识库与用户问题的回复。

### 创建RAG应用

- [ ] 通过LlamaIndex提供的高度集成工具，快速地创建一个RAG应用，并学习了如何保存和加载索引。

### RAG 多轮对话
> RAG的检索阶段：系统会将用户的问题与知识库中的文本段进行语义相似度比较，找出最相关的内容。但如果用户的问题依赖于对话历史中的上下文，会发生什么呢？
> 
> 解决方案：**问题改写**
> 如果把完整的对话历史和问题一起输入检索系统，由于文本过长，embedding模型的效果会下降。业界通用的解决方案是：
> 1. **问题改写**：利用大模型根据对话历史对当前问题进行改写，将上下文信息融入问题中
> 2. **正常检索**：使用改写后的问题进行检索和生成



## 1.2 提示词（**Prompt Engineering**）

### 1.2.1 基本要素

当和大模型在交流时，可以将它想象是一个经过“社会化训练的”人，交流方式应当和人与人之间传递信息的方式一样。你的需求需要清晰明确，不能有歧义。你的提问方式（Prompt）越清晰明确，大模型越能抓住问题的关键点，回复就越符合你的预期。为了系统性地构建高效的上下文，我们可以遵循一个包含多个基本要素的提示词框架：**任务目标、上下文、角色、受众、样例、输出格式**。这些要素共同构成了一个完整的上下文“蓝图”，能帮助你构建一个完整、有效的提示词。

|要素|含义|
|----|----|
|任务目标（Object）|明确要求大模型完成什么任务，让大模型专注具体目标|
|上下文（Context）|任务的背景信息，比如操作流程、任务场景等，明确大模型理解讨论的范围|
|角色（Role）|大模型扮演的角色，或者强调大模型应该使用的语气、写作风格等，明确大模型回应的预期情感|
|受众（Audience）|明确大模型针对的特定受众，约束大模型的应答风格|
|样例（Sample）|让大模型参考的具体案例，大模型会从中抽象出实现方案、需要注意的具体格式等信息|
|输出格式（Output Format）|明确指定输出的格式、输出类型、枚举值的范围。通常也会明确指出不需要输出的内容和不期望的信息，可以结合样例来进一步明确输出的格式和输出方法|

- [ ] 补充提示词自动优化工具

### 1.2.2 技巧

1. 清晰表达需求，并使用分隔符
2. 限定角色和受众
3. 规定输出格式（大模型有结构化输出的能力）
4. 提供少样本示例
5. 给模型“思考”的时间
	1. 思维链（COT）方法是让模型进行思考的一种方法。它通过让模型处理中间步骤，逐步将复杂问题分解为子问题，最终推导出正确答案。
	2. 使大模型进行 “思考”的方法还有很多种，比如：思维树（ToT)、思维图（GOT） 等。但是就目前大模型的发展来说，仅靠引导大模型“思考”还是无法完成更复杂的工作。大模型也逐渐从COT的提示方法向**多智能体（Agent）**方向进行发展。
6. Meta Prompting: 让大模型成为你的提示词教练
7. 多轮迭代：引入参考答案进行差距分析

## 1.3 自动化评测